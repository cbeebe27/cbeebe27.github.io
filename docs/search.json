[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Connor Beebe",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "pairsmodel.html",
    "href": "pairsmodel.html",
    "title": "Pairs Quant Model",
    "section": "",
    "text": "Preface\nDuke Energy and NextEra Energy are two of the largest utility companies in the US and both trade on the New York Stock Exchange. The industry has become much more volatile with increased regulations and pressure for these companies to decarbonize and achieve net-zero emissions. The regulatory environment is changing faster than it ever has and large capital expenditures are required of these companies to adjust. This is evident in the recent movements in share prices of industry players such as Duke and NextEra.\nA few Key points:\n\nOperate in the same industry.\nSusceptible to the same economic and market factors.\nRelationship between price movements has increased since 2020.\nPotentially cointegrated.\n\nThe above discussion points to a pairs trading strategy that takes advantage of mean reversion tactics. Throughout this document, the full process of a quantitative pairs trading model is explored in detail. Each section is meant to concisely flow through from the rationale of the model all the way to the discovery and findings after testing it.\n\n\n\n\n\n\n\n\nRationale\nPairs trading operates under the assumption that two asset prices are cointegrated through time. This means that the mean and variance are considered to be ‘time-invariant’. Above is a depiction of prices through time. I wanted to try this on something that didn’t appear to be perfect at first glance. This means that while one price may move more due to differentiating factors, there is a chance the spread will mean revert.\n\nCointegration\nCointegration is a concept that won Clive W.J. Granger a nobel prize in 2003, and is an excellent measure for pairs trading. This measure allows the construction of a single stationary time series from two asset price time series. This is done by finding the cointegration coefficient, or the cointegration beta.\nThis differs from correlation in that correlation describes the relationship between the returns, whilst cointegration is a long-term relationship between prices.\n\nReturns are normally used for analysis because they are regarded as being much closer to stationary.\nSpurious correlation is the result of a regression on unrelated prices, as prices are generally non-stationary.\nIn order to find out if the linear regression on prices produces a stationary residual, the Augmented Dickey-Fuller test can be applied to two assets that appear related.\nIf the residual is stationary, this indicates that the two asset prices are cointegrated.\nReturns will be used to calculate the hedge ratio.\n\nThe problem becomes which asset to select as the dependent variable. To solve this, run the regression twice and select the dependent variable that produces the highest significance in the ADF test. Below is an output of results from the ADF test on NEE - DUK:\n\n\n\n\n\n\n  \n    \n    \n      Test.Statistic\n      Critical.Value\n    \n  \n  \n    -2.601\n-2.57\n  \n  \n  \n\n\n\n\nSpread 1, which utilizes NEE as the dependent variable, has a more negative test statistic which is significant at the 90% confidence level. This isn’t fantastic, but it means we can assume our series is somewhat stationary and use this spread to conduct our quantitative trading strategy, utilizing a beta of 0.625 (calculated by a regression on returns) on DUK as our hedge ratio.\n\n\n\n\n\n\n\n\n\nSignals & Rules\nTo set trade signals, I’ll be using Z-score. The Z score measures how many standard deviations the spread is from the mean. For this strategy, I will use a 10 day window for mean and standard deviation.\n\nWhen the Z score crosses the upper threshold (parameter 1), Sell NEE and buy DUK (Short the spread).\nWhen the Z score crosses the lower threshold (parameter 2), Buy NEE and Sell DUK (Long the spread).\nWhen the profit-take and stop-loss thresholds (parameter 3 and 4) are crossed, it is time to exit.\nStop-loss will remain at -2.5 and 2.5.\nProfit take will remain at -0.2 and 0.2.\n\nBelow is a chart depicting the process from a one year snipped of the data for clarity. The short and long parameter values chosen are arbitrarily set and will be optimized later.\n\n\n\n\n\n\nThe logic of the model will be built so as to accommodate for all of the factors shown above, and execute positions exactly according to the signals and rules laid out.\nHere is a plot of what the strategy looks like with the set values in the plot above, over the same time frame:\n\n\n\n\n\nThe data used to observe the strategy (training data) is from January 2000 to the end of 2019.\n\n\nRisk Appetite and Desired Exposure\nI am attempting to make this model as usable and realistic as possible for me. As this is my first construction of a pairs model and I have very limited capital to deploy, I have baked in the following constraints in the trade execution logic:\n\nI only want to hold one position at a time. This means even if the z-score moves across an execution parameter before moving across the profit-take line, no new trades will be entered. This is to minimize my exposure and limit the transaction costs associated with a very high volume of trades. This constraint will help, but the nature of setting up a model like this by using the variation in z - score will still result in a fairly active model producing a high number of trades.\nStop-Loss and Profit-Take levels have been included. These constraints act to minimize risk. Stop-Loss is included so if the spread continues to move to extreme deviation I’ll cut my losses. As I am using adjusted closing prices to compute the z-score, Profit-Take levels will minimize the risk in a large movement at the open the following day, or if the z-score approaches zero but does not cross, I’ll still realize profit. These were actively included in the execution logic as well.\n\n\n\nOptimizing In Allignment With Risk Appetite\nDuring optimization, I will look to optimize the z-score values for entering into a long or short position on the spread that will maximize my cumulative return. This will be combined with minimizing the drawdowns of my strategy due to the limited capital I have available to deploy and avoid large losses. I also want to compare the percentage of winning periods to cumulative return to test for any over fitting. The exact thresholds (while maximizing Cumulative Return) are as follows:\n\nDrawdowns less than 30%.\n% win greater than 30%.\n\nAfter running iterations over different parameter values, the below charts show the optimal parameter values that maximize the value added based on my defined risk appetite indicators.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA couple thoughts based on the above plots:\n\nDrawdowns have very little variation. I believe this is due to me incorporating profit-take and stop-loss levels into the model, which combats against large hits to capital preservation.\n\n% Win higher than anticipated.\nAll objectives seem to peak in a similar area.\n\n\n\nStrategy Results and Backtesting\nAfter reviewing the data shown in the plots, the following parameter combinations hit the mark:\n\n\n\n\n\n\n  \n    \n    \n      par1\n      par2\n      CumReturn\n      Ret.Ann\n      %.Win\n      DD.Max\n    \n  \n  \n    0.6\n-2.0\n3.74\n0.08\n0.52\n-0.28\n    0.6\n-2.1\n3.71\n0.08\n0.52\n-0.26\n    0.5\n-2.1\n3.35\n0.08\n0.52\n-0.27\n  \n  \n  \n\n\n\n\nThe model wants the short trigger to happen sooner (Z = 0.6) and the long trigger slightly later (z = -2.0). Lets backtest this from 2020 onwards. The price movements since 2020 look very favorable for a strategy of this nature:\n\n\n\n\n\n\nThe performance of the position on the backtesting data:\n\n\n\n\n\nBefore looking further, lets look at transaction costs on the NYSE. 98 spread trades (196 total) were executed at a cost per transaction of $0.0012. This assumes only entering 1 spread position per transaction at a volume of 1, which I described in my risk appetite above. This is immaterial with such a low volume but if position size were to be scaled up, this could get significant.\nLets Check the stats:\n\n\n\n\n\n\n  \n    \n    \n      Metric\n      Value\n    \n  \n  \n    CumReturn\n0.12\n    Ret.Ann\n0.03\n    %.Win\n0.50\n    DD.Max\n-0.20\n  \n  \n  \n\n\n\n\nThis is compared to a Buy and Hold Return (DUK + NEE) of 29%. The returns of the pairs strategy is 14%, which is significantly lower. The 50% win indicates over fitting is not a risk here, as the model only kicked out 14% cumulative equity. This is a quality return, but I was hoping to out pace the buy and hold.\n\n\nLearnings and Review\nI tried to take on something that would be challenging and stretch my abilities. I wanted to conduct a strategy on two assets to increase the complexity of building the model, setting out the parameters, and building in the required logic to pull it off. This not only tested my trading knowledge, but also really pushed the boundaries of my coding ability. Below is a list of either concerns I have, things I learned, or assumptions I had to make simply because I could not effectively include them.\n\nThe hardest part was building a strategy function that actually worked with my parameters, as well as the profit-take and stop-loss levels that I wanted to include.\nThis model, while yet only holding one position at a time, has a very high frequency of trades. Even though I briefly discussed this, I would of liked to find a working way to include transaction costs in the model itself.\nThere are a lot of short positions. These are made under the assumption short selling is always possible, and liquidity is high enough. Finding a way to account for this would greatly improve the model.\nI tried to use stop-loss and profit-take to add complexity in place of ‘scaling in and out’. With the frequency of trades already occurring, scaling up my position would require a lot more complexity and analysis. I based this on holding one spread unit so I could focus more on having an accurate base.\nI am assuming cointegration throughout the entire time series. Ideally, I’d rather test for it over small windows and include that in my trade logic.\nI am happy that I was able to produce trade signals that opened and closed positions to my specifications.\nThe ADF test did not produce great results. I still carried out the model, but this would be great to test on a pair that I find with a better fit.\nI initially used the price beta as my hedge ratio, which resulted in a much higher level of cumulative returns. After making this adjustment and correctly using the beta found when regressing on returns, the strategy performance was less optimal, but the process is now correct, barring my logic is 100% correct when calculating returns.\n\nThis was an excellent exercise. I am happy with how much it taught me about pairs trading, quantitative modelling and learning how to actually implement and test a strategy in practice."
  }
]